# -*- coding: utf-8 -*-
"""Titanic_QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f852QcDLeEqGKoLVvxQD3bM-iXT-FhqP

# 1. Importing Libraries and the dataset
"""

import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import google.generativeai as genai

# The key part: from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain import LLMChain, PromptTemplate

# Load the Titanic dataset
df = pd.read_csv('titanic.csv')

"""# 2. Configuring Generative AI API Key

Sets up the Generative AI client with an API key for authentication.
"""

genai.configure(api_key="AIzaSyCBbEWHsAWONfy1RH_aOY8U06hWgdNB_UU")

"""# 3. Few-shot Learning

The system_template section defines the role and behavior of the assistant using a few-shot learning approach. Few-shot learning is particularly useful here because it provides the model with explicit examples of how to respond to different types of questions about the Titanic dataset. By embedding this guidance into the prompt, the assistant learns to interpret user questions, generate appropriate Python code for analysis, and decide whether graphical output is required or else numerical output.
"""

system_template = """You are a data analysis assistant that writes Python code
to analyze the Titanic dataset with the following columns- 'PassengerId', 'Survived' with binary values 0 and 1, 'Pclass' with 1(1st class), 2(2nd class), and 3(3rd class) as possible values,
'Name', 'Sex' with male and female as values, 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', and 'Embarked' with C = Cherbourg Q = Queenstown S = Southampton.

Follow these steps:
1. Figure out what the user is asking.
2. Generate a short Python code snippet that uses the Titanic dataset to answer.
3. Code can include plots if relevant (using matplotlib/seaborn).
4. Return only your code snippet in triple backticks,
   and optionally a short final text explanation or numeric answer.

Below are few-shot examples (for the Titanic dataset) to guide you:

User Question: "How many males survived?"
Thought: Filter df by Sex == 'male' and Survived == 1, then count rows.
Code: result = df[(df["Sex"] == "male") & (df["Survived"] == 1)].shape[0] print("Number of males who survived:", result)
Answer: "There were 109 males who survived." # example

---

User Question: "What percentage of survivors were females?"
Thought: Among Survived == 1, find fraction that is female.
Code: survivors = df[df["Survived"] == 1] female_count = survivors[survivors["Sex"] == "female"].shape[0] percentage = (female_count / survivors.shape[0]) * 100 print(f"Percentage of survivors who were female: {percentage:.2f}%")

Answer: "About 68.13% of survivors were females." # example

---

User Question: "How many people survived?"
Thought: Survived == 1 => count.
Code:
result = df[df["Survived"] == 1].shape[0] print("Total survivors:", result)
Answer: "There were 342 survivors." # example

---

User Question: "What is the age distribution of survivors?"
Thought: We'll generate a histogram.
Code:import matplotlib.pyplot as plt import seaborn as sns
     survivors_age = df[df["Survived"] == 1]["Age"].dropna() plt.figure(figsize=(8,5)) sns.histplot(survivors_age, kde=True) plt.title("Age Distribution of Survivors") plt.xlabel("Age") plt.ylabel("Count") plt.savefig("survivors_age_dist.png") print("Plot saved: survivors_age_dist.png")
Answer: "Plot created. Most survivors are between 20 and 40 years old." # example

---

User Question: "What was the ticket fare distribution of survivors?"
Thought: Another histogram for Fare among Survived == 1.
Code:import matplotlib.pyplot as plt import seaborn as sns
     survivors_fare = df[df["Survived"] == 1]["Fare"] plt.figure(figsize=(8,5)) sns.histplot(survivors_fare, kde=True) plt.title("Fare Distribution of Survivors") plt.xlabel("Fare") plt.ylabel("Count") plt.savefig("survivors_fare_dist.png") print("Plot saved: survivors_fare_dist.png")
Answer: "Plot created. Fares among survivors cluster under $50." # example

---

User Question: "What was the ticket fare distribution of non-survivors?"
Thought: Same approach for Survived == 0.
Code: import matplotlib.pyplot as plt import seaborn as sns
      nonsurvivors_fare = df[df["Survived"] == 0]["Fare"] plt.figure(figsize=(8,5)) sns.histplot(nonsurvivors_fare, kde=True) plt.title("Fare Distribution of Non-survivors") plt.xlabel("Fare") plt.ylabel("Count") plt.savefig("nonsurvivors_fare_dist.png") print("Plot saved: nonsurvivors_fare_dist.png")
Answer: "Plot created. Most non-survivors also had fares under $50." # example

---

Now use these examples as guidance. When I ask a new question,
generate only:
1. Code snippet in triple backticks
2. A short final answer.

If it's not related to Titanic data or you can't answer, say so.
"""

"""# 4. Chat Model and Prompt Template Initialization

This section initializes the chat model and integrates it with the prompt template:

ChatGoogleGenerativeAI: Sets up the AI model with parameters like temperature for deterministic responses.

PromptTemplate: Combines the system prompt with user queries for structured input.

LLMChain: Executes the AI model with the given prompt.
"""

chat_llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",  #using gemini 1.5 flash model
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

# This prompt template has no input variables because
# we will feed user queries as separate messages in the chain
prompt = PromptTemplate(template=system_template, input_variables=[])

llm_chain = LLMChain(
    llm=chat_llm,
    prompt=prompt,
    verbose=True
)

"""# 5. Agent Response Function

This function sends the user query to the AI model along with the system prompt. It returns the AI-generated response, which includes Python code snippets and explanations. It saves the plot images wherever necessary so that it can be displayed on the streamlit app.
"""

def agent_respond(user_query: str) -> str:
    """
    1. Provide the system prompt to ChatGoogleGenerativeAI (via the LLMChain).
    2. Provide the user query as an additional message.
    3. Return the model's text output (which should contain the code snippet in triple backticks).
    """
    # We'll "run" the chain by simply providing the user query as input:
    # But our system prompt doesn't have input variables. So we can do:

    # E.g. we can pass the user query as a separate message in the 'human' role
    # if we adapt LLMChain usage. Or do a direct invoke:

    # Option A: Direct .invoke usage with multiple messages:
    messages = [
        ("system", system_template),
        ("user", user_query),
    ]
    response_msg = chat_llm.invoke(messages)
    return response_msg.content

"""# 6. Code Extraction and Execution Functions

**extract_code_snippet**- This helper function uses a regular expression to extract Python code enclosed in triple backticks (```). This ensures only the relevant code snippet is isolated for execution.

**run_user_code**- This function executes the extracted Python code using exec. It captures and returns any output or errors during execution. The Titanic dataset and required libraries are made available in the global scope for convenience.
"""

import io
import sys

def extract_code_snippet(text: str) -> str:
    pattern = r"```(?:python)?(.*?)```"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip() if match else ""

def run_user_code(code_str: str, local_vars: dict = None) -> str:
    """
    Run the code snippet, return anything printed to stdout.
    """
    if local_vars is None:
        local_vars = {}

    # We'll attach df, etc. to the global scope for convenience
    global_vars = {"df": df, "pd": pd, "plt": plt, "sns": sns}

    # Capture prints
    captured_output = io.StringIO()
    old_stdout = sys.stdout
    sys.stdout = captured_output

    try:
        exec(code_str, global_vars, local_vars)
    except Exception as e:
        # Return error
        sys.stdout = old_stdout
        return f"Error while executing code:\n{str(e)}"
    finally:
        sys.stdout = old_stdout

    return captured_output.getvalue()

"""# 7. Displaying Saved Plots

This function checks the current directory for saved plot images (e.g., .png, .jpg) and displays them using Streamlit. This enables users to view generated visualizations directly in the app.
"""

import streamlit as st

def display_plots():
    """
    Check if any image files are saved and display them.
    """
    # Check for saved image files and display them
    plot_files = [f for f in os.listdir() if f.endswith('.png') or f.endswith('.jpg')]

    for plot in plot_files:
        st.image(plot)

"""## 8. Streamlit Application

This is the main function that drives the Streamlit app:

User Input: Collects queries related to the Titanic dataset.

AI Response: Sends the query to the agent_respond function and retrieves the AI's output.

Code Execution: Extracts and executes the AI-generated code snippet.

Result Display: Shows the code, execution output, and any saved plots to the user.
"""

def main():
    st.title("Titanic Data Q&A with LLM-generated Code")

    user_query = st.text_input("Ask a question about the Titanic dataset:")

    if st.button("Submit") and user_query.strip():
        # 1. Get LLM response (which includes code snippet + short answer)
        llm_response = agent_respond(user_query)

        # 2. Extract code snippet
        code_snippet = extract_code_snippet(llm_response)

        st.subheader("Generated Code")
        st.code(code_snippet, language="python")

        # 3. Execute code snippet
        exec_output = run_user_code(code_snippet)

        # 4. Display the result (text or error)
        st.subheader("Execution Output")
        st.text(exec_output)

        # 5. If the code saved plots, we display them
        display_plots()

"""# 9. Running the Application

This ensures the Streamlit app runs when the script is executed directly.
"""

if __name__ == "__main__":
    main()