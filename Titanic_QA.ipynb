{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Libraries"
      ],
      "metadata": {
        "id": "GRjkFItai7nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import google.generativeai as genai\n",
        "\n",
        "# The key part: from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain import LLMChain, PromptTemplate"
      ],
      "metadata": {
        "id": "n_35lCBBTpwP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configuring Generative AI API Key"
      ],
      "metadata": {
        "id": "QWuYXDk9jJVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sets up the Generative AI client with an API key for authentication."
      ],
      "metadata": {
        "id": "-1Uuf3WXjSJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"AIzaSyCBbEWHsAWONfy1RH_aOY8U06hWgdNB_UU\")"
      ],
      "metadata": {
        "id": "lVJUuFZJEwaF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Few-shot Learning"
      ],
      "metadata": {
        "id": "nHLIe6v7jZA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system_template section defines the role and behavior of the assistant using a few-shot learning approach. Few-shot learning is particularly useful here because it provides the model with explicit examples of how to respond to different types of questions about the Titanic dataset. By embedding this guidance into the prompt, the assistant learns to interpret user questions, generate appropriate Python code for analysis, and decide whether graphical output is required or else numerical output."
      ],
      "metadata": {
        "id": "RNvmjyjSjuyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"\"\"You are a data analysis assistant that writes Python code\n",
        "to analyze the Titanic dataset with the following columns- 'PassengerId', 'Survived' with binary values 0 and 1, 'Pclass' with 1(1st class), 2(2nd class), and 3(3rd class) as possible values,\n",
        "'Name', 'Sex' with male and female as values, 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', and 'Embarked' with C = Cherbourg Q = Queenstown S = Southampton.\n",
        "\n",
        "Follow these steps:\n",
        "1. Figure out what the user is asking.\n",
        "2. Generate a short Python code snippet that uses the Titanic dataset to answer.\n",
        "3. Code can include plots if relevant (using matplotlib/seaborn).\n",
        "4. Return only your code snippet in triple backticks,\n",
        "   and optionally a short final text explanation or numeric answer.\n",
        "\n",
        "Below are few-shot examples (for the Titanic dataset) to guide you:\n",
        "\n",
        "User Question: \"How many males survived?\"\n",
        "Thought: Filter df by Sex == 'male' and Survived == 1, then count rows.\n",
        "Code: result = df[(df[\"Sex\"] == \"male\") & (df[\"Survived\"] == 1)].shape[0] print(\"Number of males who survived:\", result)\n",
        "Answer: \"There were 109 males who survived.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What percentage of survivors were females?\"\n",
        "Thought: Among Survived == 1, find fraction that is female.\n",
        "Code: survivors = df[df[\"Survived\"] == 1] female_count = survivors[survivors[\"Sex\"] == \"female\"].shape[0] percentage = (female_count / survivors.shape[0]) * 100 print(f\"Percentage of survivors who were female: {percentage:.2f}%\")\n",
        "\n",
        "Answer: \"About 68.13% of survivors were females.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"How many people survived?\"\n",
        "Thought: Survived == 1 => count.\n",
        "Code:\n",
        "result = df[df[\"Survived\"] == 1].shape[0] print(\"Total survivors:\", result)\n",
        "Answer: \"There were 342 survivors.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What is the age distribution of survivors?\"\n",
        "Thought: We'll generate a histogram.\n",
        "Code:import matplotlib.pyplot as plt import seaborn as sns\n",
        "     survivors_age = df[df[\"Survived\"] == 1][\"Age\"].dropna() plt.figure(figsize=(8,5)) sns.histplot(survivors_age, kde=True) plt.title(\"Age Distribution of Survivors\") plt.xlabel(\"Age\") plt.ylabel(\"Count\") plt.savefig(\"survivors_age_dist.png\") print(\"Plot saved: survivors_age_dist.png\")\n",
        "Answer: \"Plot created. Most survivors are between 20 and 40 years old.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What was the ticket fare distribution of survivors?\"\n",
        "Thought: Another histogram for Fare among Survived == 1.\n",
        "Code:import matplotlib.pyplot as plt import seaborn as sns\n",
        "     survivors_fare = df[df[\"Survived\"] == 1][\"Fare\"] plt.figure(figsize=(8,5)) sns.histplot(survivors_fare, kde=True) plt.title(\"Fare Distribution of Survivors\") plt.xlabel(\"Fare\") plt.ylabel(\"Count\") plt.savefig(\"survivors_fare_dist.png\") print(\"Plot saved: survivors_fare_dist.png\")\n",
        "Answer: \"Plot created. Fares among survivors cluster under $50.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What was the ticket fare distribution of non-survivors?\"\n",
        "Thought: Same approach for Survived == 0.\n",
        "Code: import matplotlib.pyplot as plt import seaborn as sns\n",
        "      nonsurvivors_fare = df[df[\"Survived\"] == 0][\"Fare\"] plt.figure(figsize=(8,5)) sns.histplot(nonsurvivors_fare, kde=True) plt.title(\"Fare Distribution of Non-survivors\") plt.xlabel(\"Fare\") plt.ylabel(\"Count\") plt.savefig(\"nonsurvivors_fare_dist.png\") print(\"Plot saved: nonsurvivors_fare_dist.png\")\n",
        "Answer: \"Plot created. Most non-survivors also had fares under $50.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "Now use these examples as guidance. When I ask a new question,\n",
        "generate only:\n",
        "1. Code snippet in triple backticks\n",
        "2. A short final answer.\n",
        "\n",
        "If it's not related to Titanic data or you can't answer, say so.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "z55OP6K9ICKZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Chat Model and Prompt Template Initialization"
      ],
      "metadata": {
        "id": "0roP7BmBkhJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section initializes the chat model and integrates it with the prompt template:\n",
        "\n",
        "ChatGoogleGenerativeAI: Sets up the AI model with parameters like temperature for deterministic responses.\n",
        "\n",
        "PromptTemplate: Combines the system prompt with user queries for structured input.\n",
        "\n",
        "LLMChain: Executes the AI model with the given prompt."
      ],
      "metadata": {
        "id": "nf7i0c_Kkjid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",   # or whichever is available\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# This prompt template has no input variables because\n",
        "# we will feed user queries as separate messages in the chain\n",
        "prompt = PromptTemplate(template=system_template, input_variables=[])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=chat_llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkJy2e_QXLws",
        "outputId": "5642be88-4ac8-47fe-fd84-bc4b2bde5e7b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1e64c670e5be>:13: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Agent Response Function\n"
      ],
      "metadata": {
        "id": "Lfn3OH31kvuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function sends the user query to the AI model along with the system prompt. It returns the AI-generated response, which includes Python code snippets and explanations. It saves the plot images wherever necessary so that it can be displayed on the streamlit app."
      ],
      "metadata": {
        "id": "RndfHfcjlRSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent_respond(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    1. Provide the system prompt to ChatGoogleGenerativeAI (via the LLMChain).\n",
        "    2. Provide the user query as an additional message.\n",
        "    3. Return the model's text output (which should contain the code snippet in triple backticks).\n",
        "    \"\"\"\n",
        "    # We'll \"run\" the chain by simply providing the user query as input:\n",
        "    # But our system prompt doesn't have input variables. So we can do:\n",
        "\n",
        "    # E.g. we can pass the user query as a separate message in the 'human' role\n",
        "    # if we adapt LLMChain usage. Or do a direct invoke:\n",
        "\n",
        "    # Option A: Direct .invoke usage with multiple messages:\n",
        "    messages = [\n",
        "        (\"system\", system_template),\n",
        "        (\"user\", user_query),\n",
        "    ]\n",
        "    response_msg = chat_llm.invoke(messages)\n",
        "    return response_msg.content"
      ],
      "metadata": {
        "id": "4lz86ba0XfNv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Code Extraction and Execution Functions"
      ],
      "metadata": {
        "id": "UOwUpfP2lqcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**extract_code_snippet**- This helper function uses a regular expression to extract Python code enclosed in triple backticks (```). This ensures only the relevant code snippet is isolated for execution.\n",
        "\n",
        "**run_user_code**- This function executes the extracted Python code using exec. It captures and returns any output or errors during execution. The Titanic dataset and required libraries are made available in the global scope for convenience."
      ],
      "metadata": {
        "id": "RpxXV3v_l0d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import sys\n",
        "\n",
        "def extract_code_snippet(text: str) -> str:\n",
        "    pattern = r\"```(?:python)?(.*?)```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "\n",
        "def run_user_code(code_str: str, local_vars: dict = None) -> str:\n",
        "    \"\"\"\n",
        "    Run the code snippet, return anything printed to stdout.\n",
        "    \"\"\"\n",
        "    if local_vars is None:\n",
        "        local_vars = {}\n",
        "\n",
        "    # We'll attach df, etc. to the global scope for convenience\n",
        "    global_vars = {\"df\": df, \"pd\": pd, \"plt\": plt, \"sns\": sns}\n",
        "\n",
        "    # Capture prints\n",
        "    captured_output = io.StringIO()\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = captured_output\n",
        "\n",
        "    try:\n",
        "        exec(code_str, global_vars, local_vars)\n",
        "    except Exception as e:\n",
        "        # Return error\n",
        "        sys.stdout = old_stdout\n",
        "        return f\"Error while executing code:\\n{str(e)}\"\n",
        "    finally:\n",
        "        sys.stdout = old_stdout\n",
        "\n",
        "    return captured_output.getvalue()\n"
      ],
      "metadata": {
        "id": "pLyaTzN-YT0L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Displaying Saved Plots"
      ],
      "metadata": {
        "id": "OJKE317dmLEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function checks the current directory for saved plot images (e.g., .png, .jpg) and displays them using Streamlit. This enables users to view generated visualizations directly in the app."
      ],
      "metadata": {
        "id": "zl_TBBzvmTTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "def display_plots():\n",
        "    \"\"\"\n",
        "    Check if any image files are saved and display them.\n",
        "    \"\"\"\n",
        "    # Check for saved image files and display them\n",
        "    plot_files = [f for f in os.listdir() if f.endswith('.png') or f.endswith('.jpg')]\n",
        "\n",
        "    for plot in plot_files:\n",
        "        st.image(plot)"
      ],
      "metadata": {
        "id": "DYUmLRdOmdux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Streamlit Application\n"
      ],
      "metadata": {
        "id": "_WrXU86dme3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the main function that drives the Streamlit app:\n",
        "\n",
        "User Input: Collects queries related to the Titanic dataset.\n",
        "\n",
        "AI Response: Sends the query to the agent_respond function and retrieves the AI's output.\n",
        "\n",
        "Code Execution: Extracts and executes the AI-generated code snippet.\n",
        "\n",
        "Result Display: Shows the code, execution output, and any saved plots to the user.\n"
      ],
      "metadata": {
        "id": "OsJDr5SRmoa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    st.title(\"Titanic Data Q&A with LLM-generated Code\")\n",
        "\n",
        "    user_query = st.text_input(\"Ask a question about the Titanic dataset:\")\n",
        "\n",
        "    if st.button(\"Submit\") and user_query.strip():\n",
        "        # 1. Get LLM response (which includes code snippet + short answer)\n",
        "        llm_response = agent_respond(user_query)\n",
        "\n",
        "        # 2. Extract code snippet\n",
        "        code_snippet = extract_code_snippet(llm_response)\n",
        "\n",
        "        st.subheader(\"Generated Code\")\n",
        "        st.code(code_snippet, language=\"python\")\n",
        "\n",
        "        # 3. Execute code snippet\n",
        "        exec_output = run_user_code(code_snippet)\n",
        "\n",
        "        # 4. Display the result (text or error)\n",
        "        st.subheader(\"Execution Output\")\n",
        "        st.text(exec_output)\n",
        "\n",
        "        # 5. If the code saved plots, we display them\n",
        "        display_plots()"
      ],
      "metadata": {
        "id": "5FjOhOMtmbV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Running the Application"
      ],
      "metadata": {
        "id": "NH7TyxTnm2OZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ensures the Streamlit app runs when the script is executed directly."
      ],
      "metadata": {
        "id": "hxa-2Y5mm-4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "gt-M_goAm1DK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}