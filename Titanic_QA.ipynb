{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRjkFItai7nJ"
      },
      "source": [
        "# 1. Importing Libraries and the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n_35lCBBTpwP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import google.generativeai as genai\n",
        "\n",
        "# The key part: from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain import LLMChain, PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5rb1b_EQr2vo"
      },
      "outputs": [],
      "source": [
        "# Load the Titanic dataset\n",
        "df = pd.read_csv('titanic.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWuYXDk9jJVS"
      },
      "source": [
        "# 2. Configuring Generative AI API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Uuf3WXjSJX"
      },
      "source": [
        "Sets up the Generative AI client with an API key for authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lVJUuFZJEwaF"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=\"AIzaSyCBbEWHsAWONfy1RH_aOY8U06hWgdNB_UU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHLIe6v7jZA1"
      },
      "source": [
        "# 3. Few-shot Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNvmjyjSjuyH"
      },
      "source": [
        "The system_template section defines the role and behavior of the assistant using a few-shot learning approach. Few-shot learning is particularly useful here because it provides the model with explicit examples of how to respond to different types of questions about the Titanic dataset. By embedding this guidance into the prompt, the assistant learns to interpret user questions, generate appropriate Python code for analysis, and decide whether graphical output is required or else numerical output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z55OP6K9ICKZ"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"You are a data analysis assistant that writes Python code\n",
        "to analyze the Titanic dataset with the following columns- 'PassengerId', 'Survived' with binary values 0 and 1, 'Pclass' with 1(1st class), 2(2nd class), and 3(3rd class) as possible values,\n",
        "'Name', 'Sex' with male and female as values, 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', and 'Embarked' with C = Cherbourg Q = Queenstown S = Southampton.\n",
        "\n",
        "Follow these steps:\n",
        "1. Figure out what the user is asking.\n",
        "2. Generate a short Python code snippet that uses the Titanic dataset to answer.\n",
        "3. Code can include plots if relevant (using matplotlib/seaborn).\n",
        "4. Return only your code snippet in triple backticks,\n",
        "   and optionally a short final text explanation or numeric answer.\n",
        "\n",
        "Below are few-shot examples (for the Titanic dataset) to guide you:\n",
        "\n",
        "User Question: \"How many males survived?\"\n",
        "Thought: Filter df by Sex == 'male' and Survived == 1, then count rows.\n",
        "Code: result = df[(df[\"Sex\"] == \"male\") & (df[\"Survived\"] == 1)].shape[0] print(\"Number of males who survived:\", result)\n",
        "Answer: \"There were 109 males who survived.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What percentage of survivors were females?\"\n",
        "Thought: Among Survived == 1, find fraction that is female.\n",
        "Code: survivors = df[df[\"Survived\"] == 1] female_count = survivors[survivors[\"Sex\"] == \"female\"].shape[0] percentage = (female_count / survivors.shape[0]) * 100 print(f\"Percentage of survivors who were female: {percentage:.2f}%\")\n",
        "\n",
        "Answer: \"About 68.13% of survivors were females.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"How many people survived?\"\n",
        "Thought: Survived == 1 => count.\n",
        "Code:\n",
        "result = df[df[\"Survived\"] == 1].shape[0] print(\"Total survivors:\", result)\n",
        "Answer: \"There were 342 survivors.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What is the age distribution of survivors?\"\n",
        "Thought: We'll generate a histogram.\n",
        "Code:import matplotlib.pyplot as plt import seaborn as sns\n",
        "     survivors_age = df[df[\"Survived\"] == 1][\"Age\"].dropna() plt.figure(figsize=(8,5)) sns.histplot(survivors_age, kde=True) plt.title(\"Age Distribution of Survivors\") plt.xlabel(\"Age\") plt.ylabel(\"Count\") plt.savefig(\"survivors_age_dist.png\") print(\"Plot saved: survivors_age_dist.png\")\n",
        "Answer: \"Plot created. Most survivors are between 20 and 40 years old.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What was the ticket fare distribution of survivors?\"\n",
        "Thought: Another histogram for Fare among Survived == 1.\n",
        "Code:import matplotlib.pyplot as plt import seaborn as sns\n",
        "     survivors_fare = df[df[\"Survived\"] == 1][\"Fare\"] plt.figure(figsize=(8,5)) sns.histplot(survivors_fare, kde=True) plt.title(\"Fare Distribution of Survivors\") plt.xlabel(\"Fare\") plt.ylabel(\"Count\") plt.savefig(\"survivors_fare_dist.png\") print(\"Plot saved: survivors_fare_dist.png\")\n",
        "Answer: \"Plot created. Fares among survivors cluster under $50.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "User Question: \"What was the ticket fare distribution of non-survivors?\"\n",
        "Thought: Same approach for Survived == 0.\n",
        "Code: import matplotlib.pyplot as plt import seaborn as sns\n",
        "      nonsurvivors_fare = df[df[\"Survived\"] == 0][\"Fare\"] plt.figure(figsize=(8,5)) sns.histplot(nonsurvivors_fare, kde=True) plt.title(\"Fare Distribution of Non-survivors\") plt.xlabel(\"Fare\") plt.ylabel(\"Count\") plt.savefig(\"nonsurvivors_fare_dist.png\") print(\"Plot saved: nonsurvivors_fare_dist.png\")\n",
        "Answer: \"Plot created. Most non-survivors also had fares under $50.\" # example\n",
        "\n",
        "---\n",
        "\n",
        "Now use these examples as guidance. When I ask a new question,\n",
        "generate only:\n",
        "1. Code snippet in triple backticks\n",
        "2. A short final answer.\n",
        "\n",
        "If it's not related to Titanic data or you can't answer, say so.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0roP7BmBkhJK"
      },
      "source": [
        "# 4. Chat Model and Prompt Template Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf7i0c_Kkjid"
      },
      "source": [
        "This section initializes the chat model and integrates it with the prompt template:\n",
        "\n",
        "ChatGoogleGenerativeAI: Sets up the AI model with parameters like temperature for deterministic responses.\n",
        "\n",
        "PromptTemplate: Combines the system prompt with user queries for structured input.\n",
        "\n",
        "LLMChain: Executes the AI model with the given prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkJy2e_QXLws",
        "outputId": "5642be88-4ac8-47fe-fd84-bc4b2bde5e7b"
      },
      "outputs": [
        {
          "ename": "DefaultCredentialsError",
          "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chat_llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatGoogleGenerativeAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-1.5-flash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#using gemini 1.5 flash model\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# This prompt template has no input variables because\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# we will feed user queries as separate messages in the chain\u001b[39;00m\n\u001b[0;32m     11\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39msystem_template, input_variables\u001b[38;5;241m=\u001b[39m[])\n",
            "File \u001b[1;32mc:\\Users\\aayus\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\aayus\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:838\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    836\u001b[0m         google_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoogle_api_key\n\u001b[0;32m    837\u001b[0m transport: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport\n\u001b[1;32m--> 838\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mgenaix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_generative_service\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgoogle_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\aayus\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\_genai_extension.py:276\u001b[0m, in \u001b[0;36mbuild_generative_service\u001b[1;34m(credentials, api_key, client_options, client_info, transport)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_generative_service\u001b[39m(\n\u001b[0;32m    263\u001b[0m     credentials: Optional[credentials\u001b[38;5;241m.\u001b[39mCredentials] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    264\u001b[0m     api_key: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     transport: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m v1betaGenerativeServiceClient:\n\u001b[0;32m    269\u001b[0m     config \u001b[38;5;241m=\u001b[39m _prepare_config(\n\u001b[0;32m    270\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    271\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m         client_info\u001b[38;5;241m=\u001b[39mclient_info,\n\u001b[0;32m    275\u001b[0m     )\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1betaGenerativeServiceClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aayus\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:687\u001b[0m, in \u001b[0;36mGenerativeServiceClient.__init__\u001b[1;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[0;32m    678\u001b[0m transport_init: Union[\n\u001b[0;32m    679\u001b[0m     Type[GenerativeServiceTransport],\n\u001b[0;32m    680\u001b[0m     Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, GenerativeServiceTransport],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, GenerativeServiceTransport], transport)\n\u001b[0;32m    685\u001b[0m )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[1;32m--> 687\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m \u001b[43mtransport_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_cert_source_for_mtls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_cert_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\aayus\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:154\u001b[0m, in \u001b[0;36mGenerativeServiceGrpcTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_channel_credentials \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials(\n\u001b[0;32m    150\u001b[0m                 certificate_chain\u001b[38;5;241m=\u001b[39mcert, private_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[0;32m    151\u001b[0m             )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     channel_init \u001b[38;5;241m=\u001b[39m channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreate_channel\n",
            "File \u001b[1;32mc:\\Users\\aayus\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\base.py:100\u001b[0m, in \u001b[0;36mGenerativeServiceTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mload_credentials_from_file(\n\u001b[0;32m     97\u001b[0m         credentials_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_credentials:\n\u001b[1;32m--> 100\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_gdch_audience\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\aayus\\anaconda3\\Lib\\site-packages\\google\\auth\\_default.py:691\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    683\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    687\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[0;32m    688\u001b[0m             )\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 691\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
            "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
          ]
        }
      ],
      "source": [
        "chat_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  #using gemini 1.5 flash model\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# This prompt template has no input variables because\n",
        "# we will feed user queries as separate messages in the chain\n",
        "prompt = PromptTemplate(template=system_template, input_variables=[])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=chat_llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfn3OH31kvuM"
      },
      "source": [
        "# 5. Agent Response Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RndfHfcjlRSx"
      },
      "source": [
        "This function sends the user query to the AI model along with the system prompt. It returns the AI-generated response, which includes Python code snippets and explanations. It saves the plot images wherever necessary so that it can be displayed on the streamlit app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4lz86ba0XfNv"
      },
      "outputs": [],
      "source": [
        "def agent_respond(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    1. Provide the system prompt to ChatGoogleGenerativeAI (via the LLMChain).\n",
        "    2. Provide the user query as an additional message.\n",
        "    3. Return the model's text output (which should contain the code snippet in triple backticks).\n",
        "    \"\"\"\n",
        "    # We'll \"run\" the chain by simply providing the user query as input:\n",
        "    # But our system prompt doesn't have input variables. So we can do:\n",
        "\n",
        "    # E.g. we can pass the user query as a separate message in the 'human' role\n",
        "    # if we adapt LLMChain usage. Or do a direct invoke:\n",
        "\n",
        "    # Option A: Direct .invoke usage with multiple messages:\n",
        "    messages = [\n",
        "        (\"system\", system_template),\n",
        "        (\"user\", user_query),\n",
        "    ]\n",
        "    response_msg = chat_llm.invoke(messages)\n",
        "    return response_msg.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOwUpfP2lqcW"
      },
      "source": [
        "# 6. Code Extraction and Execution Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxXV3v_l0d4"
      },
      "source": [
        "**extract_code_snippet**- This helper function uses a regular expression to extract Python code enclosed in triple backticks (```). This ensures only the relevant code snippet is isolated for execution.\n",
        "\n",
        "**run_user_code**- This function executes the extracted Python code using exec. It captures and returns any output or errors during execution. The Titanic dataset and required libraries are made available in the global scope for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pLyaTzN-YT0L"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import sys\n",
        "\n",
        "def extract_code_snippet(text: str) -> str:\n",
        "    pattern = r\"```(?:python)?(.*?)```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    return match.group(1).strip() if match else \"\"\n",
        "\n",
        "def run_user_code(code_str: str, local_vars: dict = None) -> str:\n",
        "    \"\"\"\n",
        "    Run the code snippet, return anything printed to stdout.\n",
        "    \"\"\"\n",
        "    if local_vars is None:\n",
        "        local_vars = {}\n",
        "\n",
        "    # We'll attach df, etc. to the global scope for convenience\n",
        "    global_vars = {\"df\": df, \"pd\": pd, \"plt\": plt, \"sns\": sns}\n",
        "\n",
        "    # Capture prints\n",
        "    captured_output = io.StringIO()\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = captured_output\n",
        "\n",
        "    try:\n",
        "        exec(code_str, global_vars, local_vars)\n",
        "    except Exception as e:\n",
        "        # Return error\n",
        "        sys.stdout = old_stdout\n",
        "        return f\"Error while executing code:\\n{str(e)}\"\n",
        "    finally:\n",
        "        sys.stdout = old_stdout\n",
        "\n",
        "    return captured_output.getvalue()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJKE317dmLEW"
      },
      "source": [
        "# 7. Displaying Saved Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl_TBBzvmTTy"
      },
      "source": [
        "This function checks the current directory for saved plot images (e.g., .png, .jpg) and displays them using Streamlit. This enables users to view generated visualizations directly in the app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYUmLRdOmdux"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "\n",
        "def display_plots():\n",
        "    \"\"\"\n",
        "    Check if any image files are saved and display them.\n",
        "    \"\"\"\n",
        "    # Check for saved image files and display them\n",
        "    plot_files = [f for f in os.listdir() if f.endswith('.png') or f.endswith('.jpg')]\n",
        "\n",
        "    for plot in plot_files:\n",
        "        st.image(plot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WrXU86dme3g"
      },
      "source": [
        "## 8. Streamlit Application\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsJDr5SRmoa2"
      },
      "source": [
        "This is the main function that drives the Streamlit app:\n",
        "\n",
        "User Input: Collects queries related to the Titanic dataset.\n",
        "\n",
        "AI Response: Sends the query to the agent_respond function and retrieves the AI's output.\n",
        "\n",
        "Code Execution: Extracts and executes the AI-generated code snippet.\n",
        "\n",
        "Result Display: Shows the code, execution output, and any saved plots to the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FjOhOMtmbV7"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    st.title(\"Titanic Data Q&A with LLM-generated Code\")\n",
        "\n",
        "    user_query = st.text_input(\"Ask a question about the Titanic dataset:\")\n",
        "\n",
        "    if st.button(\"Submit\") and user_query.strip():\n",
        "        # 1. Get LLM response (which includes code snippet + short answer)\n",
        "        llm_response = agent_respond(user_query)\n",
        "\n",
        "        # 2. Extract code snippet\n",
        "        code_snippet = extract_code_snippet(llm_response)\n",
        "\n",
        "        st.subheader(\"Generated Code\")\n",
        "        st.code(code_snippet, language=\"python\")\n",
        "\n",
        "        # 3. Execute code snippet\n",
        "        exec_output = run_user_code(code_snippet)\n",
        "\n",
        "        # 4. Display the result (text or error)\n",
        "        st.subheader(\"Execution Output\")\n",
        "        st.text(exec_output)\n",
        "\n",
        "        # 5. If the code saved plots, we display them\n",
        "        display_plots()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH7TyxTnm2OZ"
      },
      "source": [
        "# 9. Running the Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxa-2Y5mm-4X"
      },
      "source": [
        "This ensures the Streamlit app runs when the script is executed directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt-M_goAm1DK"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
